{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aff2c9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     16\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, RFE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to create directory if it doesn't exist\n",
    "def create_dir_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "def load_feature_data(feature_file_path):\n",
    "    \"\"\"Load feature data from CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(feature_file_path)\n",
    "        print(f\"Loaded data with {df.shape[0]} samples and {df.shape[1]} features\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_data_for_modeling(df):\n",
    "    \"\"\"Prepare data for modeling by separating features and target\"\"\"\n",
    "    # Extract character name as target\n",
    "    X = df.copy()\n",
    "    \n",
    "    # Assuming 'char_name' contains the Bengali character labels\n",
    "    y = X['char_name'] if 'char_name' in X.columns else X['file_name']\n",
    "    \n",
    "    # Remove non-feature columns\n",
    "    non_feature_cols = ['file_name', 'char_name']\n",
    "    X = X.drop([col for col in non_feature_cols if col in X.columns], axis=1)\n",
    "    \n",
    "    # Encode the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Handle any remaining non-numeric columns\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X = X.drop(col, axis=1)\n",
    "    \n",
    "    # Fill NaN values with column means\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, X_scaled, y, y_encoded, label_encoder, X.columns\n",
    "\n",
    "def plot_correlation_heatmap(X, feature_names, output_dir):\n",
    "    \"\"\"Generate a correlation heatmap of features.\"\"\"\n",
    "    # Create DataFrame with feature names\n",
    "    df_corr = pd.DataFrame(X, columns=feature_names)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df_corr.corr()\n",
    "    \n",
    "    # Create mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(max(10, len(feature_names) * 0.5), max(8, len(feature_names) * 0.5)))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        cmap='coolwarm',\n",
    "        annot=False,\n",
    "        square=True,\n",
    "        linewidths=.5,\n",
    "        cbar_kws={\"shrink\": .5},\n",
    "        xticklabels=corr_matrix.columns,\n",
    "        yticklabels=corr_matrix.columns\n",
    "    )\n",
    "    \n",
    "    # Improve layout\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.title('Feature Correlation Heatmap', fontsize=18, pad=20)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'correlation_heatmap.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved correlation heatmap to {output_path}\")\n",
    "\n",
    "def plot_random_forest_importance(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate a random forest feature importance plot\"\"\"\n",
    "    # Train Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y_encoded)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Select top 30 features for readability\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_indices = indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Random Forest Feature Importance', fontsize=16)\n",
    "    plt.bar(range(top_n), importances[top_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'random_forest_importance.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved random forest importance plot to {output_path}\")\n",
    "    \n",
    "    return rf  # Return the model for later use\n",
    "\n",
    "def plot_pca_scatter(X_scaled, y_encoded, label_encoder, output_dir):\n",
    "    \"\"\"Generate 2D and 3D PCA scatter plots\"\"\"\n",
    "    # 2D PCA\n",
    "    pca_2d = PCA(n_components=2)\n",
    "    X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot 2D PCA\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Get unique classes\n",
    "    unique_classes = np.unique(y_encoded)\n",
    "    \n",
    "    # Create a colormap\n",
    "    cmap = plt.cm.get_cmap('tab20', len(unique_classes))\n",
    "    \n",
    "    # Plot each class\n",
    "    for i, class_idx in enumerate(unique_classes):\n",
    "        plt.scatter(X_pca_2d[y_encoded == class_idx, 0], \n",
    "                    X_pca_2d[y_encoded == class_idx, 1],\n",
    "                    color=cmap(i), \n",
    "                    alpha=0.7,\n",
    "                    label=f\"{label_encoder.inverse_transform([class_idx])[0]}\")\n",
    "    \n",
    "    plt.title('2D PCA of Features', fontsize=16)\n",
    "    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    \n",
    "    # Add legend if not too many classes\n",
    "    if len(unique_classes) <= 20:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save 2D plot\n",
    "    output_path_2d = os.path.join(output_dir, 'pca_2d_scatter.png')\n",
    "    plt.savefig(output_path_2d, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved 2D PCA scatter plot to {output_path_2d}\")\n",
    "    \n",
    "    # 3D PCA\n",
    "    pca_3d = PCA(n_components=3)\n",
    "    X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot 3D PCA\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot each class\n",
    "    for i, class_idx in enumerate(unique_classes):\n",
    "        ax.scatter(X_pca_3d[y_encoded == class_idx, 0],\n",
    "                   X_pca_3d[y_encoded == class_idx, 1],\n",
    "                   X_pca_3d[y_encoded == class_idx, 2],\n",
    "                   color=cmap(i),\n",
    "                   alpha=0.7,\n",
    "                   label=f\"{label_encoder.inverse_transform([class_idx])[0]}\")\n",
    "    \n",
    "    ax.set_title('3D PCA of Features', fontsize=16)\n",
    "    ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%} variance)')\n",
    "    \n",
    "    # Add legend if not too many classes\n",
    "    if len(unique_classes) <= 20:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save 3D plot\n",
    "    output_path_3d = os.path.join(output_dir, 'pca_3d_scatter.png')\n",
    "    plt.savefig(output_path_3d, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved 3D PCA scatter plot to {output_path_3d}\")\n",
    "\n",
    "def plot_permutation_importance(model, X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate permutation feature importance plot\"\"\"\n",
    "    # Calculate permutation importance\n",
    "    result = permutation_importance(model, X_scaled, y_encoded, n_repeats=10, random_state=42)\n",
    "    perm_importance = result.importances_mean\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(perm_importance)[::-1]\n",
    "    \n",
    "    # Select top 30 features for readability\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_indices = indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Permutation Feature Importance', fontsize=16)\n",
    "    plt.bar(range(top_n), perm_importance[top_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'permutation_importance.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved permutation importance plot to {output_path}\")\n",
    "\n",
    "def plot_shap_summary(model, X_scaled, feature_names, output_dir):\n",
    "    \"\"\"Generate SHAP values summary plot\"\"\"\n",
    "    # Create a smaller subset if dataset is large (SHAP can be computationally intensive)\n",
    "    max_samples = min(500, X_scaled.shape[0])\n",
    "    X_sample = X_scaled[:max_samples]\n",
    "    \n",
    "    try:\n",
    "        # SHAP explainer for Random Forest\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # If multi-class, take the mean of absolute SHAP values across classes\n",
    "        if isinstance(shap_values, list):\n",
    "            # Average across all classes for feature importance\n",
    "            shap_values_mean = np.abs(np.array(shap_values)).mean(axis=0)\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Create DataFrame for SHAP summary plot\n",
    "            shap_df = pd.DataFrame(shap_values_mean, columns=feature_names)\n",
    "            \n",
    "            # Get feature importance based on mean absolute SHAP values\n",
    "            feature_importance = np.mean(np.abs(shap_values_mean), axis=0)\n",
    "            sorted_idx = np.argsort(feature_importance)\n",
    "            \n",
    "            # Select top 30 features\n",
    "            top_n = min(30, len(feature_names))\n",
    "            top_idx = sorted_idx[-top_n:]\n",
    "            \n",
    "            # Plot\n",
    "            plt.barh(range(top_n), feature_importance[top_idx])\n",
    "            plt.yticks(range(top_n), [feature_names[i] for i in top_idx])\n",
    "            plt.xlabel('Mean |SHAP value|')\n",
    "            plt.title('Feature Importance based on SHAP Values')\n",
    "            \n",
    "            # Save the plot\n",
    "            output_path = os.path.join(output_dir, 'shap_summary.png')\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Also create a traditional SHAP summary plot for a single class if not too many features\n",
    "            if len(feature_names) <= 30:\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                # Use the first class for the summary plot\n",
    "                shap.summary_plot(shap_values[0], pd.DataFrame(X_sample, columns=feature_names), \n",
    "                                 show=False, max_display=20)\n",
    "                plt.title('SHAP Summary Plot (First Class)', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                # Save the plot\n",
    "                output_path = os.path.join(output_dir, 'shap_summary_first_class.png')\n",
    "                plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        else:\n",
    "            # Binary classification case\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            shap.summary_plot(shap_values, pd.DataFrame(X_sample, columns=feature_names), \n",
    "                             show=False, max_display=20)\n",
    "            plt.title('SHAP Summary Plot', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the plot\n",
    "            output_path = os.path.join(output_dir, 'shap_summary.png')\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"Saved SHAP summary plot to {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating SHAP plot: {e}\")\n",
    "\n",
    "def plot_univariate_importance(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate univariate feature importance plots using F-test and Mutual Information\"\"\"\n",
    "    # F-test\n",
    "    f_values, p_values = f_classif(X_scaled, y_encoded)\n",
    "    \n",
    "    # Sort features by F-values\n",
    "    f_indices = np.argsort(f_values)[::-1]\n",
    "    \n",
    "    # Select top 30 features for readability\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_f_indices = f_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Univariate Feature Importance (F-test)', fontsize=16)\n",
    "    plt.bar(range(top_n), f_values[top_f_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_f_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the F-test plot\n",
    "    output_path_f = os.path.join(output_dir, 'f_test_importance.png')\n",
    "    plt.savefig(output_path_f, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved F-test importance plot to {output_path_f}\")\n",
    "    \n",
    "    # Mutual Information\n",
    "    mi_values = mutual_info_classif(X_scaled, y_encoded, random_state=42)\n",
    "    \n",
    "    # Sort features by MI values\n",
    "    mi_indices = np.argsort(mi_values)[::-1]\n",
    "    \n",
    "    # Select top 30 features\n",
    "    top_mi_indices = mi_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Univariate Feature Importance (Mutual Information)', fontsize=16)\n",
    "    plt.bar(range(top_n), mi_values[top_mi_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_mi_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the MI plot\n",
    "    output_path_mi = os.path.join(output_dir, 'mutual_info_importance.png')\n",
    "    plt.savefig(output_path_mi, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved Mutual Information importance plot to {output_path_mi}\")\n",
    "    \n",
    "    return f_values, mi_values\n",
    "\n",
    "def plot_rfe_ranking(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate Recursive Feature Elimination (RFE) ranking plot\"\"\"\n",
    "    # Create a classifier for RFE\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    \n",
    "    # Use a subset of features if there are too many\n",
    "    n_features_to_select = min(30, len(feature_names))\n",
    "    \n",
    "    # Run RFE\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_features_to_select, step=1)\n",
    "    rfe.fit(X_scaled, y_encoded)\n",
    "    \n",
    "    # Get feature ranking (the smaller the number, the more important the feature)\n",
    "    ranking = rfe.ranking_\n",
    "    \n",
    "    # Sort features by ranking\n",
    "    ranking_indices = np.argsort(ranking)\n",
    "    \n",
    "    # Select top 30 features\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_ranking_indices = ranking_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Recursive Feature Elimination (RFE) Ranking', fontsize=16)\n",
    "    plt.bar(range(top_n), [1/r if r > 0 else float('inf') for r in ranking[top_ranking_indices]], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_ranking_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.ylabel('1/Ranking (higher is more important)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'rfe_ranking.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved RFE ranking plot to {output_path}\")\n",
    "\n",
    "def plot_lasso_coefficients(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate LASSO coefficient plot for sparse feature selection\"\"\"\n",
    "    # For multiclass, use one-vs-rest approach\n",
    "    n_classes = len(np.unique(y_encoded))\n",
    "    \n",
    "    # Create binary target variables for One-vs-Rest\n",
    "    y_binary = np.zeros((len(y_encoded), n_classes))\n",
    "    for i in range(n_classes):\n",
    "        y_binary[:, i] = (y_encoded == i).astype(int)\n",
    "    \n",
    "    # Train LASSO model for each class\n",
    "    alpha = 0.01  # Adjust if needed\n",
    "    coefs = np.zeros((n_classes, X_scaled.shape[1]))\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        # Train LASSO for this class\n",
    "        lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "        lasso.fit(X_scaled, y_binary[:, i])\n",
    "        coefs[i, :] = lasso.coef_\n",
    "    \n",
    "    # Compute average absolute coefficient for each feature across classes\n",
    "    avg_abs_coefs = np.mean(np.abs(coefs), axis=0)\n",
    "    \n",
    "    # Sort features by coefficient magnitude\n",
    "    coef_indices = np.argsort(avg_abs_coefs)[::-1]\n",
    "    \n",
    "    # Select top features\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_coef_indices = coef_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('LASSO Coefficient Magnitude (Averaged Across Classes)', fontsize=16)\n",
    "    plt.bar(range(top_n), avg_abs_coefs[top_coef_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_coef_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'lasso_coefficients.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved LASSO coefficient plot to {output_path}\")\n",
    "\n",
    "def create_feature_importance_summary(feature_names, rf_importances, perm_importances, \n",
    "                                     f_values, mi_values, output_dir):\n",
    "    \"\"\"Create a summary CSV of feature importances from different methods\"\"\"\n",
    "    # Create a DataFrame to hold importance values\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names,\n",
    "                                  'RandomForest': rf_importances,\n",
    "                                  'Permutation': perm_importances,\n",
    "                                  'F_Test': f_values,\n",
    "                                  'MutualInfo': mi_values})\n",
    "    \n",
    "    # Sort by average rank across methods\n",
    "    # First, create rank columns (higher value = more important)\n",
    "    for col in ['RandomForest', 'Permutation', 'F_Test', 'MutualInfo']:\n",
    "        importance_df[f'{col}_Rank'] = importance_df[col].rank(ascending=False)\n",
    "    \n",
    "    # Calculate average rank\n",
    "    rank_cols = [c for c in importance_df.columns if c.endswith('_Rank')]\n",
    "    importance_df['AvgRank'] = importance_df[rank_cols].mean(axis=1)\n",
    "    \n",
    "    # Sort by average rank\n",
    "    importance_df = importance_df.sort_values('AvgRank')\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(output_dir, 'feature_importance_summary.csv')\n",
    "    importance_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved feature importance summary to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to coordinate visualization tasks\"\"\"\n",
    "    # Define input and output directories\n",
    "    feature_file = \"featureExtraction/all_features.csv\"\n",
    "    output_dir = \"visualizations\"\n",
    "    \n",
    "    # Create output directory\n",
    "    create_dir_if_not_exists(output_dir)\n",
    "    \n",
    "    # Load feature data\n",
    "    df = load_feature_data(feature_file)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Prepare data for modeling\n",
    "    X, X_scaled, y, y_encoded, label_encoder, feature_names = prepare_data_for_modeling(df)\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    plot_correlation_heatmap(X, feature_names, output_dir)\n",
    "    \n",
    "    # Plot Random Forest feature importance\n",
    "    print(\"\\nGenerating random forest feature importance...\")\n",
    "    rf_model = plot_random_forest_importance(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot PCA scatter plots\n",
    "    print(\"\\nGenerating PCA scatter plots...\")\n",
    "    plot_pca_scatter(X_scaled, y_encoded, label_encoder, output_dir)\n",
    "    \n",
    "    # Plot permutation feature importance\n",
    "    print(\"\\nGenerating permutation feature importance...\")\n",
    "    result = permutation_importance(rf_model, X_scaled, y_encoded, n_repeats=10, random_state=42)\n",
    "    perm_importances = result.importances_mean\n",
    "    plot_permutation_importance(rf_model, X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot SHAP values summary\n",
    "    print(\"\\nGenerating SHAP values summary plot...\")\n",
    "    plot_shap_summary(rf_model, X_scaled, feature_names, output_dir)\n",
    "    \n",
    "    # Plot univariate feature importance\n",
    "    print(\"\\nGenerating univariate feature importance plots...\")\n",
    "    f_values, mi_values = plot_univariate_importance(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot RFE ranking\n",
    "    print(\"\\nGenerating Recursive Feature Elimination (RFE) ranking plot...\")\n",
    "    plot_rfe_ranking(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot LASSO coefficients\n",
    "    print(\"\\nGenerating LASSO coefficient plot...\")\n",
    "    plot_lasso_coefficients(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Create feature importance summary\n",
    "    print(\"\\nCreating feature importance summary...\")\n",
    "    create_feature_importance_summary(feature_names, \n",
    "                                     rf_model.feature_importances_,\n",
    "                                     perm_importances,\n",
    "                                     f_values,\n",
    "                                     mi_values,\n",
    "                                     output_dir)\n",
    "    \n",
    "    print(\"\\nAll visualizations completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b93b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: visualizations\n",
      "Loaded data with 283 samples and 109 features\n",
      "\n",
      "Generating correlation heatmap...\n",
      "Saved correlation heatmap to visualizations\\correlation_heatmap.png\n",
      "\n",
      "Generating random forest feature importance...\n",
      "Saved random forest importance plot to visualizations\\random_forest_importance.png\n",
      "\n",
      "Generating PCA scatter plots...\n",
      "Saved 2D PCA scatter plot to visualizations\\pca_2d_scatter.png\n",
      "Saved 3D PCA scatter plot to visualizations\\pca_3d_scatter.png\n",
      "\n",
      "Generating permutation feature importance...\n",
      "Saved permutation importance plot to visualizations\\permutation_importance.png\n",
      "\n",
      "Generating feature importance visualization (SHAP alternative)...\n",
      "Saved feature importance variation plot to visualizations\\feature_importance_with_variation.png\n",
      "Could not create partial dependence plot: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\inspection\\__init__.py)\n",
      "\n",
      "Generating univariate feature importance plots...\n",
      "Saved F-test importance plot to visualizations\\f_test_importance.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 539\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll visualizations completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 539\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[5], line 517\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Plot univariate feature importance\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating univariate feature importance plots...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 517\u001b[0m f_values, mi_values \u001b[38;5;241m=\u001b[39m plot_univariate_importance(X_scaled, y_encoded, feature_names, output_dir)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Plot RFE ranking\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating Recursive Feature Elimination (RFE) ranking plot...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 349\u001b[0m, in \u001b[0;36mplot_univariate_importance\u001b[1;34m(X_scaled, y_encoded, feature_names, output_dir)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved F-test importance plot to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path_f\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# Mutual Information\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m mi_values \u001b[38;5;241m=\u001b[39m mutual_info_classif(X_scaled, y_encoded, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# Sort features by MI values\u001b[39;00m\n\u001b[0;32m    352\u001b[0m mi_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(mi_values)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:514\u001b[0m, in \u001b[0;36mmutual_info_classif\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information for a discrete target variable.\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03mMutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m       0.     , 0.     , 0.     , 0.      , 0.        ])\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m check_classification_targets(y)\n\u001b[1;32m--> 514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _estimate_mi(X, y, discrete_features, \u001b[38;5;28;01mTrue\u001b[39;00m, n_neighbors, copy, random_state)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:305\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    297\u001b[0m     y \u001b[38;5;241m=\u001b[39m scale(y, with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    298\u001b[0m     y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;241m1e-10\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(y)))\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;241m*\u001b[39m rng\u001b[38;5;241m.\u001b[39mstandard_normal(size\u001b[38;5;241m=\u001b[39mn_samples)\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    304\u001b[0m mi \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 305\u001b[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, discrete_feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[0;32m    307\u001b[0m ]\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(mi)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:166\u001b[0m, in \u001b[0;36m_compute_mi\u001b[1;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_mi_cd(y, x, n_neighbors)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x_discrete \u001b[38;5;129;01mand\u001b[39;00m y_discrete:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_mi_cd(x, y, n_neighbors)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_mi_cc(x, y, n_neighbors)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:141\u001b[0m, in \u001b[0;36m_compute_mi_cd\u001b[1;34m(c, d, n_neighbors)\u001b[0m\n\u001b[0;32m    138\u001b[0m c \u001b[38;5;241m=\u001b[39m c[mask]\n\u001b[0;32m    139\u001b[0m radius \u001b[38;5;241m=\u001b[39m radius[mask]\n\u001b[1;32m--> 141\u001b[0m kd \u001b[38;5;241m=\u001b[39m KDTree(c)\n\u001b[0;32m    142\u001b[0m m_all \u001b[38;5;241m=\u001b[39m kd\u001b[38;5;241m.\u001b[39mquery_radius(c, radius, count_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    143\u001b[0m m_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(m_all)\n",
      "File \u001b[1;32msklearn\\\\neighbors\\\\_binary_tree.pxi:900\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree64.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, RFE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to create directory if it doesn't exist\n",
    "def create_dir_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "def load_feature_data(feature_file_path):\n",
    "    \"\"\"Load feature data from CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(feature_file_path)\n",
    "        print(f\"Loaded data with {df.shape[0]} samples and {df.shape[1]} features\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_data_for_modeling(df):\n",
    "    \"\"\"Prepare data for modeling by separating features and target\"\"\"\n",
    "    # Extract character name as target\n",
    "    X = df.copy()\n",
    "    \n",
    "    # Assuming 'char_name' contains the Bengali character labels\n",
    "    y = X['char_name'] if 'char_name' in X.columns else X['file_name']\n",
    "    \n",
    "    # Remove non-feature columns\n",
    "    non_feature_cols = ['file_name', 'char_name']\n",
    "    X = X.drop([col for col in non_feature_cols if col in X.columns], axis=1)\n",
    "    \n",
    "    # Encode the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Handle any remaining non-numeric columns\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X = X.drop(col, axis=1)\n",
    "    \n",
    "    # Fill NaN values with column means\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, X_scaled, y, y_encoded, label_encoder, X.columns\n",
    "\n",
    "def plot_correlation_heatmap(X, feature_names, output_dir):\n",
    "    \"\"\"Generate a correlation heatmap of features\"\"\"\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    \n",
    "    # Create DataFrame for correlation\n",
    "    df_corr = pd.DataFrame(X, columns=feature_names)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df_corr.corr()\n",
    "    \n",
    "    # Create mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=False, \n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "    plt.title('Feature Correlation Heatmap', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'correlation_heatmap.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved correlation heatmap to {output_path}\")\n",
    "\n",
    "def plot_random_forest_importance(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate a random forest feature importance plot\"\"\"\n",
    "    # Train Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y_encoded)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Select top 30 features for readability\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_indices = indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Random Forest Feature Importance', fontsize=16)\n",
    "    plt.bar(range(top_n), importances[top_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'random_forest_importance.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved random forest importance plot to {output_path}\")\n",
    "    \n",
    "    return rf  # Return the model for later use\n",
    "\n",
    "def plot_pca_scatter(X_scaled, y_encoded, label_encoder, output_dir):\n",
    "    \"\"\"Generate 2D and 3D PCA scatter plots\"\"\"\n",
    "    # 2D PCA\n",
    "    pca_2d = PCA(n_components=2)\n",
    "    X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot 2D PCA\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Get unique classes\n",
    "    unique_classes = np.unique(y_encoded)\n",
    "    \n",
    "    # Create a colormap\n",
    "    cmap = plt.cm.get_cmap('tab20', len(unique_classes))\n",
    "    \n",
    "    # Plot each class\n",
    "    for i, class_idx in enumerate(unique_classes):\n",
    "        plt.scatter(X_pca_2d[y_encoded == class_idx, 0], \n",
    "                    X_pca_2d[y_encoded == class_idx, 1],\n",
    "                    color=cmap(i), \n",
    "                    alpha=0.7,\n",
    "                    label=f\"{label_encoder.inverse_transform([class_idx])[0]}\")\n",
    "    \n",
    "    plt.title('2D PCA of Features', fontsize=16)\n",
    "    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    \n",
    "    # Add legend if not too many classes\n",
    "    if len(unique_classes) <= 20:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save 2D plot\n",
    "    output_path_2d = os.path.join(output_dir, 'pca_2d_scatter.png')\n",
    "    plt.savefig(output_path_2d, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved 2D PCA scatter plot to {output_path_2d}\")\n",
    "    \n",
    "    # 3D PCA\n",
    "    pca_3d = PCA(n_components=3)\n",
    "    X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot 3D PCA\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot each class\n",
    "    for i, class_idx in enumerate(unique_classes):\n",
    "        ax.scatter(X_pca_3d[y_encoded == class_idx, 0],\n",
    "                   X_pca_3d[y_encoded == class_idx, 1],\n",
    "                   X_pca_3d[y_encoded == class_idx, 2],\n",
    "                   color=cmap(i),\n",
    "                   alpha=0.7,\n",
    "                   label=f\"{label_encoder.inverse_transform([class_idx])[0]}\")\n",
    "    \n",
    "    ax.set_title('3D PCA of Features', fontsize=16)\n",
    "    ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%} variance)')\n",
    "    \n",
    "    # Add legend if not too many classes\n",
    "    if len(unique_classes) <= 20:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save 3D plot\n",
    "    output_path_3d = os.path.join(output_dir, 'pca_3d_scatter.png')\n",
    "    plt.savefig(output_path_3d, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved 3D PCA scatter plot to {output_path_3d}\")\n",
    "\n",
    "def plot_permutation_importance(model, X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate permutation feature importance plot\"\"\"\n",
    "    # Calculate permutation importance\n",
    "    result = permutation_importance(model, X_scaled, y_encoded, n_repeats=10, random_state=42)\n",
    "    perm_importance = result.importances_mean\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(perm_importance)[::-1]\n",
    "    \n",
    "    # Select top 30 features for readability\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_indices = indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Permutation Feature Importance', fontsize=16)\n",
    "    plt.bar(range(top_n), perm_importance[top_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'permutation_importance.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved permutation importance plot to {output_path}\")\n",
    "    \n",
    "    return perm_importance\n",
    "\n",
    "def plot_feature_importance_heatmap(model, X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate a feature importance heatmap as an alternative to SHAP plots\"\"\"\n",
    "    # Get the number of classes\n",
    "    n_classes = len(np.unique(y_encoded))\n",
    "    \n",
    "    # For small number of classes, we can create class-specific importances\n",
    "    if n_classes <= 10:\n",
    "        class_importances = {}\n",
    "        \n",
    "        # Compute separate permutation importance for each class\n",
    "        for class_idx in range(n_classes):\n",
    "            # Create binary target for this class (one-vs-rest)\n",
    "            y_binary = (y_encoded == class_idx).astype(int)\n",
    "            \n",
    "            # Calculate permutation importance for this binary problem\n",
    "            result = permutation_importance(\n",
    "                model, X_scaled, y_binary, \n",
    "                n_repeats=5, random_state=42\n",
    "            )\n",
    "            class_importances[class_idx] = result.importances_mean\n",
    "        \n",
    "        # Get top N features by average importance across classes\n",
    "        avg_importances = np.zeros(len(feature_names))\n",
    "        for class_idx in range(n_classes):\n",
    "            avg_importances += class_importances[class_idx]\n",
    "        avg_importances /= n_classes\n",
    "        \n",
    "        # Get top features\n",
    "        top_n = min(20, len(feature_names))\n",
    "        top_indices = np.argsort(avg_importances)[::-1][:top_n]\n",
    "        \n",
    "        # Create a matrix for heatmap (features x classes)\n",
    "        importance_matrix = np.zeros((top_n, n_classes))\n",
    "        for i, feature_idx in enumerate(top_indices):\n",
    "            for class_idx in range(n_classes):\n",
    "                importance_matrix[i, class_idx] = class_importances[class_idx][feature_idx]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(importance_matrix, annot=True, cmap='viridis', fmt='.3f',\n",
    "                   yticklabels=[feature_names[i] for i in top_indices],\n",
    "                   xticklabels=[f'Class {i}' for i in range(n_classes)])\n",
    "        plt.title('Feature Importance by Class (Permutation Importance)', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        output_path = os.path.join(output_dir, 'feature_importance_by_class.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved feature importance by class heatmap to {output_path}\")\n",
    "    \n",
    "    # Feature importance variation plot (alternative to SHAP summary)\n",
    "    # This shows mean and standard deviation of feature importance\n",
    "    result = permutation_importance(model, X_scaled, y_encoded, n_repeats=20, random_state=42)\n",
    "    \n",
    "    # Get top N features\n",
    "    top_n = min(30, len(feature_names))\n",
    "    perm_sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "    top_indices = perm_sorted_idx[:top_n]\n",
    "    \n",
    "    # Create plot showing mean and std of importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(top_n), result.importances_mean[top_indices],\n",
    "           yerr=result.importances_std[top_indices],\n",
    "           align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_indices], rotation=90)\n",
    "    plt.title('Feature Importance with Variation (Alternative to SHAP)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'feature_importance_with_variation.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved feature importance variation plot to {output_path}\")\n",
    "    \n",
    "    # Plot partial dependence plots for top features (another SHAP alternative)\n",
    "    try:\n",
    "        from sklearn.inspection import plot_partial_dependence\n",
    "        \n",
    "        # Use top 5 features for partial dependence\n",
    "        top_5_indices = top_indices[:5]\n",
    "        top_5_features = [feature_names[i] for i in top_5_indices]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Plot partial dependence\n",
    "        display = plot_partial_dependence(\n",
    "            model, X_scaled, features=top_5_indices, \n",
    "            feature_names=feature_names, n_jobs=2,\n",
    "            ax=ax, random_state=42\n",
    "        )\n",
    "        \n",
    "        plt.suptitle('Partial Dependence of Top Features (SHAP Alternative)', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        output_path = os.path.join(output_dir, 'partial_dependence.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved partial dependence plot to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create partial dependence plot: {e}\")\n",
    "\n",
    "def plot_univariate_importance(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate univariate feature importance plots using F-test and Mutual Information\"\"\"\n",
    "    # F-test\n",
    "    f_values, p_values = f_classif(X_scaled, y_encoded)\n",
    "    \n",
    "    # Sort features by F-values\n",
    "    f_indices = np.argsort(f_values)[::-1]\n",
    "    \n",
    "    # Select top 30 features for readability\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_f_indices = f_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Univariate Feature Importance (F-test)', fontsize=16)\n",
    "    plt.bar(range(top_n), f_values[top_f_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_f_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the F-test plot\n",
    "    output_path_f = os.path.join(output_dir, 'f_test_importance.png')\n",
    "    plt.savefig(output_path_f, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved F-test importance plot to {output_path_f}\")\n",
    "    \n",
    "    # Mutual Information\n",
    "    mi_values = mutual_info_classif(X_scaled, y_encoded, random_state=42)\n",
    "    \n",
    "    # Sort features by MI values\n",
    "    mi_indices = np.argsort(mi_values)[::-1]\n",
    "    \n",
    "    # Select top 30 features\n",
    "    top_mi_indices = mi_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Univariate Feature Importance (Mutual Information)', fontsize=16)\n",
    "    plt.bar(range(top_n), mi_values[top_mi_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_mi_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the MI plot\n",
    "    output_path_mi = os.path.join(output_dir, 'mutual_info_importance.png')\n",
    "    plt.savefig(output_path_mi, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved Mutual Information importance plot to {output_path_mi}\")\n",
    "    \n",
    "    return f_values, mi_values\n",
    "\n",
    "def plot_rfe_ranking(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate Recursive Feature Elimination (RFE) ranking plot\"\"\"\n",
    "    # Create a classifier for RFE\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    \n",
    "    # Use a subset of features if there are too many\n",
    "    n_features_to_select = min(30, len(feature_names))\n",
    "    \n",
    "    # Run RFE\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_features_to_select, step=1)\n",
    "    rfe.fit(X_scaled, y_encoded)\n",
    "    \n",
    "    # Get feature ranking (the smaller the number, the more important the feature)\n",
    "    ranking = rfe.ranking_\n",
    "    \n",
    "    # Sort features by ranking\n",
    "    ranking_indices = np.argsort(ranking)\n",
    "    \n",
    "    # Select top 30 features\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_ranking_indices = ranking_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Recursive Feature Elimination (RFE) Ranking', fontsize=16)\n",
    "    plt.bar(range(top_n), [1/r if r > 0 else float('inf') for r in ranking[top_ranking_indices]], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_ranking_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.ylabel('1/Ranking (higher is more important)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'rfe_ranking.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved RFE ranking plot to {output_path}\")\n",
    "\n",
    "def plot_lasso_coefficients(X_scaled, y_encoded, feature_names, output_dir):\n",
    "    \"\"\"Generate LASSO coefficient plot for sparse feature selection\"\"\"\n",
    "    # For multiclass, use one-vs-rest approach\n",
    "    n_classes = len(np.unique(y_encoded))\n",
    "    \n",
    "    # Create binary target variables for One-vs-Rest\n",
    "    y_binary = np.zeros((len(y_encoded), n_classes))\n",
    "    for i in range(n_classes):\n",
    "        y_binary[:, i] = (y_encoded == i).astype(int)\n",
    "    \n",
    "    # Train LASSO model for each class\n",
    "    alpha = 0.01  # Adjust if needed\n",
    "    coefs = np.zeros((n_classes, X_scaled.shape[1]))\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        # Train LASSO for this class\n",
    "        lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "        lasso.fit(X_scaled, y_binary[:, i])\n",
    "        coefs[i, :] = lasso.coef_\n",
    "    \n",
    "    # Compute average absolute coefficient for each feature across classes\n",
    "    avg_abs_coefs = np.mean(np.abs(coefs), axis=0)\n",
    "    \n",
    "    # Sort features by coefficient magnitude\n",
    "    coef_indices = np.argsort(avg_abs_coefs)[::-1]\n",
    "    \n",
    "    # Select top features\n",
    "    top_n = min(30, len(feature_names))\n",
    "    top_coef_indices = coef_indices[:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('LASSO Coefficient Magnitude (Averaged Across Classes)', fontsize=16)\n",
    "    plt.bar(range(top_n), avg_abs_coefs[top_coef_indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in top_coef_indices], rotation=90)\n",
    "    plt.xlim([-1, top_n])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, 'lasso_coefficients.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved LASSO coefficient plot to {output_path}\")\n",
    "\n",
    "def create_feature_importance_summary(feature_names, rf_importances, perm_importances, \n",
    "                                     f_values, mi_values, output_dir):\n",
    "    \"\"\"Create a summary CSV of feature importances from different methods\"\"\"\n",
    "    # Create a DataFrame to hold importance values\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names,\n",
    "                                  'RandomForest': rf_importances,\n",
    "                                  'Permutation': perm_importances,\n",
    "                                  'F_Test': f_values,\n",
    "                                  'MutualInfo': mi_values})\n",
    "    \n",
    "    # Sort by average rank across methods\n",
    "    # First, create rank columns (higher value = more important)\n",
    "    for col in ['RandomForest', 'Permutation', 'F_Test', 'MutualInfo']:\n",
    "        importance_df[f'{col}_Rank'] = importance_df[col].rank(ascending=False)\n",
    "    \n",
    "    # Calculate average rank\n",
    "    rank_cols = [c for c in importance_df.columns if c.endswith('_Rank')]\n",
    "    importance_df['AvgRank'] = importance_df[rank_cols].mean(axis=1)\n",
    "    \n",
    "    # Sort by average rank\n",
    "    importance_df = importance_df.sort_values('AvgRank')\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(output_dir, 'feature_importance_summary.csv')\n",
    "    importance_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved feature importance summary to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to coordinate visualization tasks\"\"\"\n",
    "    # Define input and output directories\n",
    "    feature_file = \"featureExtraction/all_features.csv\"\n",
    "    output_dir = \"visualizations\"\n",
    "    \n",
    "    # Create output directory\n",
    "    create_dir_if_not_exists(output_dir)\n",
    "    \n",
    "    # Load feature data\n",
    "    df = load_feature_data(feature_file)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Prepare data for modeling\n",
    "    X, X_scaled, y, y_encoded, label_encoder, feature_names = prepare_data_for_modeling(df)\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    plot_correlation_heatmap(X, feature_names, output_dir)\n",
    "    \n",
    "    # Plot Random Forest feature importance\n",
    "    print(\"\\nGenerating random forest feature importance...\")\n",
    "    rf_model = plot_random_forest_importance(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot PCA scatter plots\n",
    "    print(\"\\nGenerating PCA scatter plots...\")\n",
    "    plot_pca_scatter(X_scaled, y_encoded, label_encoder, output_dir)\n",
    "    \n",
    "    # Plot permutation feature importance\n",
    "    print(\"\\nGenerating permutation feature importance...\")\n",
    "    perm_importances = plot_permutation_importance(rf_model, X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot feature importance alternatives to SHAP\n",
    "    print(\"\\nGenerating feature importance visualization (SHAP alternative)...\")\n",
    "    plot_feature_importance_heatmap(rf_model, X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot univariate feature importance\n",
    "    print(\"\\nGenerating univariate feature importance plots...\")\n",
    "    f_values, mi_values = plot_univariate_importance(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot RFE ranking\n",
    "    print(\"\\nGenerating Recursive Feature Elimination (RFE) ranking plot...\")\n",
    "    plot_rfe_ranking(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Plot LASSO coefficients\n",
    "    print(\"\\nGenerating LASSO coefficient plot...\")\n",
    "    plot_lasso_coefficients(X_scaled, y_encoded, feature_names, output_dir)\n",
    "    \n",
    "    # Create feature importance summary\n",
    "    print(\"\\nCreating feature importance summary...\")\n",
    "    create_feature_importance_summary(feature_names, \n",
    "                                     rf_model.feature_importances_,\n",
    "                                     perm_importances,\n",
    "                                     f_values,\n",
    "                                     mi_values,\n",
    "                                     output_dir)\n",
    "    \n",
    "    print(\"\\nAll visualizations completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive feature analysis...\n",
      "Successfully loaded data with 283 rows and 109 columns.\n",
      "Data preprocessed: 107 features available.\n",
      "\n",
      "Generating reports...\n",
      "\n",
      "Correlation heatmap saved to correlation_heatmap.png\n",
      "Random Forest Feature Importance plot saved to random_forest_importance.png\n",
      "PCA 2D scatter plot saved to pca_2d_scatter.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 326\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll analyses completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 326\u001b[0m     run_all_analyses()\n",
      "Cell \u001b[1;32mIn[4], line 315\u001b[0m, in \u001b[0;36mrun_all_analyses\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    313\u001b[0m generate_correlation_heatmap(features)\n\u001b[0;32m    314\u001b[0m random_forest_importance(features, target)\n\u001b[1;32m--> 315\u001b[0m generate_pca_plots(features, target)\n\u001b[0;32m    316\u001b[0m permutation_importance_plot(features, target)\n\u001b[0;32m    317\u001b[0m univariate_feature_importance(features, target, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 183\u001b[0m, in \u001b[0;36mgenerate_pca_plots\u001b[1;34m(features, target, output_file_2d, output_file_3d)\u001b[0m\n\u001b[0;32m    181\u001b[0m ax\u001b[38;5;241m.\u001b[39mlegend(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharacters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    182\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m--> 183\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(output_file_3d, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    184\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA 3D scatter plot saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file_3d\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:1134\u001b[0m, in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m res \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\figure.py:3390\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3388\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m   3389\u001b[0m         _recursively_make_axes_transparent(stack, ax)\n\u001b[1;32m-> 3390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:2193\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2193\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2194\u001b[0m             filename,\n\u001b[0;32m   2195\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2196\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2197\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2198\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2199\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2200\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:2043\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2042\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2046\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:497\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_pil(filename_or_obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m, pil_kwargs, metadata)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:445\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    441\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    446\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    447\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    448\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m--> 387\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer)\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\contextlib.py:772\u001b[0m, in \u001b[0;36mnullcontext.__init__\u001b[1;34m(self, enter_result)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mnullcontext\u001b[39;00m(AbstractContextManager, AbstractAsyncContextManager):\n\u001b[0;32m    762\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager that does no additional processing.\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m    Used as a stand-in for a normal context manager, when a particular\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;124;03m        # Perform operation, using optional_cm if condition is True\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, enter_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menter_result \u001b[38;5;241m=\u001b[39m enter_result\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the style for the plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_path='all_features.csv'):\n",
    "    \"\"\"Load the feature data from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded data with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the data for analysis.\"\"\"\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Extract target variable from char_name column\n",
    "    target = df_processed['char_name']\n",
    "    \n",
    "    # Remove non-feature columns\n",
    "    features = df_processed.drop(['file_name', 'char_name'], axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    features = features.fillna(features.mean())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(features),\n",
    "        columns=features.columns\n",
    "    )\n",
    "    \n",
    "    print(f\"Data preprocessed: {features_scaled.shape[1]} features available.\")\n",
    "    return features_scaled, target\n",
    "\n",
    "def generate_correlation_heatmap(features, output_file='correlation_heatmap.png'):\n",
    "    \"\"\"Generate and save correlation heatmap.\"\"\"\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = features.corr()\n",
    "    \n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        cmap='coolwarm', \n",
    "        annot=False,\n",
    "        linewidths=.5, \n",
    "        center=0,\n",
    "        square=True,\n",
    "        vmin=-1, \n",
    "        vmax=1\n",
    "    )\n",
    "    \n",
    "    plt.title('Feature Correlation Heatmap', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Correlation heatmap saved to {output_file}\")\n",
    "\n",
    "def random_forest_importance(features, target, output_file='random_forest_importance.png'):\n",
    "    \"\"\"Generate and save Random Forest Feature Importance plot.\"\"\"\n",
    "    # Train a Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(features, target)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot the top 30 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(30), importances[indices][:30], align='center')\n",
    "    plt.xticks(range(30), features.columns[indices][:30], rotation=90)\n",
    "    plt.title('Random Forest Feature Importance', fontsize=16)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Random Forest Feature Importance plot saved to {output_file}\")\n",
    "\n",
    "def generate_pca_plots(features, target, output_file_2d='pca_2d_scatter.png', output_file_3d='pca_3d_scatter.png'):\n",
    "    \"\"\"Generate and save PCA 2D and 3D scatter plots.\"\"\"\n",
    "    # Prepare colormap for different classes\n",
    "    unique_targets = target.unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_targets)))\n",
    "    \n",
    "    # 2D PCA Plot\n",
    "    pca_2d = PCA(n_components=2)\n",
    "    pca_result_2d = pca_2d.fit_transform(features)\n",
    "    \n",
    "    # Create a DataFrame with PCA results\n",
    "    pca_df_2d = pd.DataFrame(data={'PCA1': pca_result_2d[:, 0], 'PCA2': pca_result_2d[:, 1], 'Target': target})\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, target_class in enumerate(unique_targets):\n",
    "        indices = pca_df_2d['Target'] == target_class\n",
    "        plt.scatter(\n",
    "            pca_df_2d.loc[indices, 'PCA1'], \n",
    "            pca_df_2d.loc[indices, 'PCA2'],\n",
    "            c=[colors[i]],\n",
    "            label=target_class,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    explained_var_2d = pca_2d.explained_variance_ratio_\n",
    "    plt.title(f'PCA 2D Scatter Plot\\nExplained Variance: {explained_var_2d[0]:.2%} (PC1), {explained_var_2d[1]:.2%} (PC2)', fontsize=16)\n",
    "    plt.xlabel(f'Principal Component 1 ({explained_var_2d[0]:.2%})')\n",
    "    plt.ylabel(f'Principal Component 2 ({explained_var_2d[1]:.2%})')\n",
    "    plt.legend(title=\"Characters\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file_2d, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"PCA 2D scatter plot saved to {output_file_2d}\")\n",
    "    \n",
    "    # 3D PCA Plot\n",
    "    pca_3d = PCA(n_components=3)\n",
    "    pca_result_3d = pca_3d.fit_transform(features)\n",
    "    \n",
    "    # Create a DataFrame with PCA results\n",
    "    pca_df_3d = pd.DataFrame(\n",
    "        data={'PCA1': pca_result_3d[:, 0], \n",
    "              'PCA2': pca_result_3d[:, 1], \n",
    "              'PCA3': pca_result_3d[:, 2], \n",
    "              'Target': target}\n",
    "    )\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for i, target_class in enumerate(unique_targets):\n",
    "        indices = pca_df_3d['Target'] == target_class\n",
    "        ax.scatter(\n",
    "            pca_df_3d.loc[indices, 'PCA1'],\n",
    "            pca_df_3d.loc[indices, 'PCA2'],\n",
    "            pca_df_3d.loc[indices, 'PCA3'],\n",
    "            c=[colors[i]],\n",
    "            label=target_class,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    explained_var_3d = pca_3d.explained_variance_ratio_\n",
    "    ax.set_title(f'PCA 3D Scatter Plot\\nTotal Explained Variance: {sum(explained_var_3d):.2%}', fontsize=16)\n",
    "    ax.set_xlabel(f'PC1 ({explained_var_3d[0]:.2%})')\n",
    "    ax.set_ylabel(f'PC2 ({explained_var_3d[1]:.2%})')\n",
    "    ax.set_zlabel(f'PC3 ({explained_var_3d[2]:.2%})')\n",
    "    ax.legend(title=\"Characters\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file_3d, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"PCA 3D scatter plot saved to {output_file_3d}\")\n",
    "\n",
    "def permutation_importance_plot(features, target, output_file='permutation_importance.png'):\n",
    "    \"\"\"Generate and save Permutation Feature Importance plot.\"\"\"\n",
    "    # Train a Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(features, target)\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    perm_importance = permutation_importance(rf, features, target, n_repeats=10, random_state=42)\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
    "    \n",
    "    # Plot the top 30 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.boxplot(\n",
    "        perm_importance.importances[sorted_idx][:30].T,\n",
    "        vert=False,\n",
    "        labels=features.columns[sorted_idx][:30]\n",
    "    )\n",
    "    plt.title(\"Permutation Feature Importance\", fontsize=16)\n",
    "    plt.xlabel(\"Decrease in Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Permutation Feature Importance plot saved to {output_file}\")\n",
    "\n",
    "def univariate_feature_importance(features, target, method='f_test', output_file='univariate_importance.png'):\n",
    "    \"\"\"Generate and save Univariate Feature Importance plot.\"\"\"\n",
    "    # Choose the scoring function\n",
    "    if method == 'f_test':\n",
    "        score_func = f_classif\n",
    "        title = 'Univariate Feature Importance (F-test)'\n",
    "    else:  # mutual_info\n",
    "        score_func = mutual_info_classif\n",
    "        title = 'Univariate Feature Importance (Mutual Information)'\n",
    "    \n",
    "    # Apply SelectKBest\n",
    "    selector = SelectKBest(score_func=score_func, k='all')\n",
    "    selector.fit(features, target)\n",
    "    \n",
    "    # Get scores and sort them\n",
    "    scores = selector.scores_\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    \n",
    "    # Plot the top 30 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(30), scores[indices][:30], align='center')\n",
    "    plt.xticks(range(30), features.columns[indices][:30], rotation=90)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Univariate Feature Importance plot saved to {output_file}\")\n",
    "\n",
    "def rfe_ranking_plot(features, target, output_file='rfe_ranking.png'):\n",
    "    \"\"\"Generate and save Recursive Feature Elimination Ranking plot.\"\"\"\n",
    "    # Initialize the RF classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Create RFE object and rank features\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=10, step=1)\n",
    "    rfe.fit(features, target)\n",
    "    \n",
    "    # Get feature ranking\n",
    "    ranking = rfe.ranking_\n",
    "    \n",
    "    # Sort features by ranking\n",
    "    indices = np.argsort(ranking)\n",
    "    \n",
    "    # Plot the top 30 features with lowest rank (most important)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(30), [ranking[i] for i in indices[:30]])\n",
    "    plt.yticks(range(30), features.columns[indices][:30])\n",
    "    plt.title('Recursive Feature Elimination Ranking', fontsize=16)\n",
    "    plt.xlabel('Rank (lower is better)')\n",
    "    plt.ylabel('Features')\n",
    "    plt.gca().invert_xaxis()  # Invert x-axis to show best features on top\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"RFE Ranking plot saved to {output_file}\")\n",
    "\n",
    "def lasso_coefficient_plot(features, target, output_file='lasso_coefficients.png'):\n",
    "    \"\"\"Generate and save LASSO Coefficient plot.\"\"\"\n",
    "    # Convert target to numerical values for regression\n",
    "    target_encoder = {label: i for i, label in enumerate(target.unique())}\n",
    "    y_numeric = target.map(target_encoder)\n",
    "    \n",
    "    # Fit LASSO model\n",
    "    lasso = Lasso(alpha=0.01)\n",
    "    lasso.fit(features, y_numeric)\n",
    "    \n",
    "    # Get coefficients and sort by absolute value\n",
    "    coefs = lasso.coef_\n",
    "    indices = np.argsort(np.abs(coefs))[::-1]\n",
    "    \n",
    "    # Plot the top 30 features with highest absolute coefficient\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(30), coefs[indices][:30], align='center')\n",
    "    plt.xticks(range(30), features.columns[indices][:30], rotation=90)\n",
    "    plt.title('LASSO Coefficients (Alpha=0.01)', fontsize=16)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"LASSO Coefficient plot saved to {output_file}\")\n",
    "\n",
    "def run_all_analyses(file_path='all_features.csv'):\n",
    "    \"\"\"Run all feature analysis and generate reports.\"\"\"\n",
    "    print(\"Starting comprehensive feature analysis...\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = load_data(file_path)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Preprocess the data\n",
    "    features, target = preprocess_data(df)\n",
    "    \n",
    "    # Generate all reports\n",
    "    print(\"\\nGenerating reports...\\n\")\n",
    "    \n",
    "    generate_correlation_heatmap(features)\n",
    "    random_forest_importance(features, target)\n",
    "    generate_pca_plots(features, target)\n",
    "    permutation_importance_plot(features, target)\n",
    "    univariate_feature_importance(features, target, method='f_test')\n",
    "    univariate_feature_importance(features, target, method='mutual_info', \n",
    "                               output_file='mutual_info_importance.png')\n",
    "    rfe_ranking_plot(features, target)\n",
    "    lasso_coefficient_plot(features, target)\n",
    "    \n",
    "    print(\"\\nAll analyses completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_analyses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
